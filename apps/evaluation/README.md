# Fine-Tuning Data Safety Evaluation

This notebook evaluates the safety of fine-tuning data by using GPT-3.5 and GPT-4 to check if the responses generated by a language model are appropriate for a young child.

## Dependencies

The notebook requires the following dependencies:

- `transformers`
- `torch`
- `openai`
- `pandas`
- `requests`
- `tqdm`

You can install the necessary packages by running:

```bash
pip install accelerate==0.21.0 openai==1.25.0 pandas==2.2.1
```

## Usage

This notebook should be run in Colab ONLY.

1. Make sure you have the required dependencies installed and set your `HF_TOKEN` and `OPENAI_API_KEY` as Colab secrets.

2. Set the `RUN_SAFETY_CHECKS` variable to `True` if you want to perform safety checks on the fine-tuning data.

3. Run the notebook cells in order.

4. The notebook will download the fine-tuning data from the specified GitHub repository and save it locally.

5. If safety checks are enabled, the notebook will evaluate the safety of each prompt-completion pair using GPT-3.5 and GPT-4. The results will be added to the DataFrame and saved as CSV files.

6. You can analyze the safety check results in the generated CSV files (`df_gpt_3.csv` and `df_gpt_4.csv`).

Note: Running safety checks using GPT-3.5 and GPT-4 may take a considerable amount of time depending on the size of the fine-tuning data.

## Workflow

1. The notebook loads a pre-trained model (`mistralai/Mistral-7B-Instruct-v0.1`) using the `AutoModelForCausalLM` and `AutoTokenizer` classes from the `transformers` library.

2. It defines utility functions for generating responses from the loaded model (`model_generate`) and GPT-3.5/GPT-4 (`gpt_generate`).

3. The `gpt_check_is_safe` function is defined to check if a given prompt-completion pair is safe for a young child. It uses GPT-3.5 or GPT-4 to determine the safety and provides a rationale for the decision.

4. The notebook downloads fine-tuning data from a GitHub repository and saves it locally as JSON files.

5. The JSON files are loaded into a pandas DataFrame (`df`) for further processing.

6. If `RUN_SAFETY_CHECKS` is set to `True`, the notebook performs safety checks on the fine-tuning data using GPT-3.5 and GPT-4. It iterates over each row of the DataFrame and calls the `gpt_check_is_safe` function to evaluate the safety of the prompt-completion pair.

7. The safety check results (answer and rationale) are added as new columns to the DataFrame.

8. Finally, the DataFrame with safety check results is saved as a CSV file (`df_gpt_3.csv` for GPT-3.5 and `df_gpt_4.csv` for GPT-4).
